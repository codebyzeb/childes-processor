{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CHILDES Tokenizer\n",
    "\n",
    "Using the phonemes in our CHILDES dataset, we train a tokenizer that just splits according to whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/6tzh0bsj2txd1cz18gpcms_c0000gn/T/ipykernel_39914/3157729071.py:1: DtypeWarning: Columns (4,7,8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  phoible = pd.read_csv('../data/phoible.csv')\n"
     ]
    }
   ],
   "source": [
    "phoible = pd.read_csv('../data/phoible.csv')\n",
    "phoible_phonemes = phoible.Phoneme.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 10\n",
    "STRESS_RE = re.compile(r\"[ˈˌ'-]+\")\n",
    "\n",
    "def build_vocabulary(datasets, column='ipa_transcription', allow_non_phoible=False, allow_stressed_tokens=False):\n",
    "\n",
    "    vocab = {'UNK' : 0, 'PAD' : 1, 'WORD_BOUNDARY' : 2, 'UTT_BOUNDARY' : 3}\n",
    "    unk_tokens = []\n",
    "    token_counts = {}\n",
    "    for dataset in datasets:\n",
    "        for line in dataset[column]:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                if token not in token_counts:\n",
    "                    token_counts[token] = 0\n",
    "                token_counts[token] += 1\n",
    "        \n",
    "    # Add tokens to vocab if they are not in phoible and have a count greater than MIN_COUNT\n",
    "    for token, count in token_counts.items():\n",
    "        if count > MIN_COUNT and token not in vocab:\n",
    "            if token not in phoible_phonemes and not allow_non_phoible:\n",
    "                if allow_stressed_tokens and STRESS_RE.findall(token):\n",
    "                    vocab[token] = len(vocab)\n",
    "                else:\n",
    "                    unk_tokens.append(token)\n",
    "            else:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "    print('Tokens not found in phoible: ', {token: token_counts[token] for token in unk_tokens})\n",
    "    print('Vocab: ', vocab)\n",
    "    print('Vocab size: ', len(vocab))\n",
    "    return vocab\n",
    "\n",
    "def build_phoneme_tokenizer(vocab, add_stress_replacer=False):\n",
    "\n",
    "    # We replace any kind of stress marker with a single primary stress marker\n",
    "    norms = []\n",
    "    if add_stress_replacer:\n",
    "        new_vocab = {}\n",
    "        for token in vocab:\n",
    "            if STRESS_RE.findall(token):\n",
    "                new_token = \"ˈ\" + STRESS_RE.sub('', token)\n",
    "                if token != new_token:\n",
    "                    norms.append(normalizers.Replace(token, new_token))\n",
    "                token = new_token\n",
    "            if token not in new_vocab:\n",
    "                new_vocab[token] = len(new_vocab)\n",
    "        vocab = new_vocab\n",
    "        print('Using only primary stress markers...')\n",
    "        print('New vocab: ', vocab)\n",
    "        print('New vocab size: ', len(vocab))\n",
    "    norms.append(normalizers.Strip())\n",
    "\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "    # tokenizer.normalizer = normalizers.Sequence([normalizers.Replace(' WORD_BOUNDARY', ''), normalizers.Strip()]) \n",
    "    tokenizer.normalizer = normalizers.Sequence(norms) \n",
    "    tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"UTT_BOUNDARY\", \"WORD_BOUNDARY\"])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"UTT_BOUNDARY $A\",\n",
    "        pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "        special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    "    )\n",
    "\n",
    "    wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')\n",
    "    return wrapped_tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer for each language in CHILDES\n",
    "\n",
    "We create a unique tokenizer for each language, to keep the vocabulary size appropriate for each language. For most languages we remove any tokens not found in Phoible. We do not do this for Mandarin or Cantonese as for these languages we merge the tone marker and preceding vowel into one phoneme, whereas Phoible treats tone markers as independent symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: ['EnglishNA', 'EnglishUK', 'French', 'German', 'Spanish', 'Dutch', 'Mandarin', 'Japanese', 'Cantonese', 'Estonian', 'Croatian', 'Danish', 'Basque', 'Hungarian', 'Turkish', 'Farsi', 'Icelandic', 'Indonesian', 'Irish', 'Welsh', 'Korean', 'Swedish', 'Norwegian', 'Quechua', 'Catalan', 'Italian', 'PortuguesePt', 'PortugueseBr', 'Romanian', 'Serbian', 'Polish']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 856M/856M [01:03<00:00, 13.5MB/s] \n",
      "Generating train split: 2564614 examples [00:13, 183649.41 examples/s]\n",
      "Downloading data: 100%|██████████| 649M/649M [00:55<00:00, 11.7MB/s] \n",
      "Generating train split: 2043115 examples [00:09, 206074.50 examples/s]\n",
      "Downloading data: 100%|██████████| 267M/267M [00:29<00:00, 8.90MB/s] \n",
      "Generating train split: 721121 examples [00:04, 153748.52 examples/s]\n",
      "Downloading data: 100%|██████████| 544M/544M [00:52<00:00, 10.4MB/s] \n",
      "Generating train split: 1525559 examples [00:08, 172756.08 examples/s]\n",
      "Downloading data: 100%|██████████| 191M/191M [00:18<00:00, 10.5MB/s] \n",
      "Generating train split: 533308 examples [00:02, 179323.77 examples/s]\n",
      "Downloading data: 100%|██████████| 130M/130M [00:12<00:00, 10.3MB/s] \n",
      "Generating train split: 403472 examples [00:02, 200529.49 examples/s]\n",
      "Downloading data: 100%|██████████| 209M/209M [00:19<00:00, 10.6MB/s] \n",
      "Generating train split: 530022 examples [00:03, 140328.05 examples/s]\n",
      "Downloading data: 100%|██████████| 295M/295M [00:30<00:00, 9.79MB/s] \n",
      "Generating train split: 998642 examples [00:04, 217190.23 examples/s]\n",
      "Downloading data: 100%|██████████| 72.0M/72.0M [00:08<00:00, 8.23MB/s]\n",
      "Generating train split: 205729 examples [00:01, 167455.24 examples/s]\n",
      "Downloading data: 100%|██████████| 71.7M/71.7M [00:07<00:00, 9.03MB/s]\n",
      "Generating train split: 186921 examples [00:01, 169121.14 examples/s]\n",
      "Downloading data: 100%|██████████| 27.9M/27.9M [00:03<00:00, 8.38MB/s]\n",
      "Generating train split: 90992 examples [00:00, 188599.34 examples/s]\n",
      "Downloading data: 100%|██████████| 24.7M/24.7M [00:03<00:00, 7.79MB/s]\n",
      "Generating train split: 84019 examples [00:00, 217691.80 examples/s]\n",
      "Downloading data: 100%|██████████| 22.4M/22.4M [00:02<00:00, 9.24MB/s]\n",
      "Generating train split: 71537 examples [00:00, 187454.62 examples/s]\n",
      "Downloading data: 100%|██████████| 23.0M/23.0M [00:03<00:00, 6.56MB/s]\n",
      "Generating train split: 69690 examples [00:00, 171198.69 examples/s]\n",
      "Downloading data: 100%|██████████| 9.13M/9.13M [00:01<00:00, 7.84MB/s]\n",
      "Generating train split: 29317 examples [00:00, 165024.99 examples/s]\n",
      "Downloading data: 100%|██████████| 5.36M/5.36M [00:00<00:00, 5.74MB/s]\n",
      "Generating train split: 22613 examples [00:00, 232557.53 examples/s]\n",
      "Downloading data: 100%|██████████| 26.8M/26.8M [00:03<00:00, 8.92MB/s]\n",
      "Generating train split: 78181 examples [00:00, 155748.71 examples/s]\n",
      "Downloading data: 100%|██████████| 243M/243M [00:24<00:00, 10.0MB/s] \n",
      "Generating train split: 813795 examples [00:03, 213132.42 examples/s]\n",
      "Downloading data: 100%|██████████| 9.85M/9.85M [00:01<00:00, 7.80MB/s]\n",
      "Generating train split: 27818 examples [00:00, 164321.51 examples/s]\n",
      "Downloading data: 100%|██████████| 56.9M/56.9M [00:05<00:00, 10.4MB/s]\n",
      "Generating train split: 181292 examples [00:00, 201147.20 examples/s]\n",
      "Downloading data: 100%|██████████| 31.1M/31.1M [00:03<00:00, 8.25MB/s]\n",
      "Generating train split: 105281 examples [00:00, 192056.66 examples/s]\n",
      "Downloading data: 100%|██████████| 50.2M/50.2M [00:05<00:00, 9.02MB/s]\n",
      "Generating train split: 154064 examples [00:00, 186853.90 examples/s]\n",
      "Downloading data: 100%|██████████| 20.1M/20.1M [00:02<00:00, 9.20MB/s]\n",
      "Generating train split: 61906 examples [00:00, 181541.84 examples/s]\n",
      "Downloading data: 100%|██████████| 6.11M/6.11M [00:00<00:00, 6.19MB/s]\n",
      "Generating train split: 22397 examples [00:00, 204206.80 examples/s]\n",
      "Downloading data: 100%|██████████| 28.7M/28.7M [00:03<00:00, 9.40MB/s]\n",
      "Generating train split: 89103 examples [00:00, 201799.10 examples/s]\n",
      "Downloading data: 100%|██████████| 31.8M/31.8M [00:03<00:00, 9.96MB/s]\n",
      "Generating train split: 94361 examples [00:00, 185969.99 examples/s]\n",
      "Downloading data: 100%|██████████| 44.0M/44.0M [00:04<00:00, 9.24MB/s]\n",
      "Generating train split: 134543 examples [00:00, 182762.24 examples/s]\n",
      "Downloading data: 100%|██████████| 12.6M/12.6M [00:06<00:00, 2.04MB/s]\n",
      "Generating train split: 22439 examples [00:00, 98236.62 examples/s]\n",
      "Downloading data: 100%|██████████| 15.7M/15.7M [00:10<00:00, 1.48MB/s]\n",
      "Generating train split: 54982 examples [00:00, 213588.64 examples/s]\n",
      "Downloading data: 100%|██████████| 102M/102M [00:15<00:00, 6.52MB/s] \n",
      "Generating train split: 319305 examples [00:01, 184303.86 examples/s]\n",
      "Downloading data: 100%|██████████| 91.4M/91.4M [00:08<00:00, 11.1MB/s]\n",
      "Generating train split: 218860 examples [00:01, 136433.46 examples/s]\n"
     ]
    }
   ],
   "source": [
    "languages = get_dataset_config_names('phonemetransformers/IPA-CHILDES')\n",
    "print('Languages:', languages)\n",
    "datasets = {language : load_dataset('phonemetransformers/IPA-CHILDES', language, split='train') for language in languages}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training tokenizer for EnglishNA...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd̠ʒ': 4, 'ʌ': 5, 's': 6, 't': 7, 'l': 8, 'aɪ': 9, 'k': 10, 'j': 11, 'ʊ': 12, 'ɹ': 13, 'b': 14, 'æ': 15, 'h': 16, 'oʊ': 17, 'm': 18, 'iː': 19, 'ð': 20, 'ɛ': 21, 'z': 22, 'f': 23, 'eɪ': 24, 'w': 25, 'ɪ': 26, 'ɡ': 27, 'ɑ': 28, 'ə': 29, 'p': 30, 'uː': 31, 'i': 32, 'θ': 33, 'ŋ': 34, 'ɔ': 35, 'ɔɪ': 36, 'n': 37, 'd': 38, 'aʊ': 39, 'v': 40, 'ɜː': 41, 't̠ʃ': 42, 'ʃ': 43, 'iə': 44, 'ʒ': 45, 'x': 46}\n",
      "Vocab size:  47\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd̠ʒ': 4, 'ʌ': 5, 's': 6, 't': 7, 'l': 8, 'aɪ': 9, 'k': 10, 'j': 11, 'ʊ': 12, 'ɹ': 13, 'b': 14, 'æ': 15, 'h': 16, 'oʊ': 17, 'm': 18, 'iː': 19, 'ð': 20, 'ɛ': 21, 'z': 22, 'f': 23, 'eɪ': 24, 'w': 25, 'ɪ': 26, 'ɡ': 27, 'ɑ': 28, 'ə': 29, 'p': 30, 'uː': 31, 'i': 32, 'θ': 33, 'ŋ': 34, 'ɔ': 35, 'ɔɪ': 36, 'n': 37, 'd': 38, 'aʊ': 39, 'v': 40, 'ɜː': 41, 't̠ʃ': 42, 'ʃ': 43, 'iə': 44, 'ʒ': 45, 'x': 46}\n",
      "New vocab size:  47\n",
      "Tokenizer for EnglishNA saved.\n",
      "\n",
      "Training tokenizer for EnglishUK...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ð': 4, 'æ': 5, 'tʰ': 6, 'ɡ': 7, 'ʊ': 8, 'd': 9, 'ɑː': 10, 'l': 11, 'ɪ': 12, 'n': 13, 'eɪ': 14, 't̠ʃ': 15, 'w': 16, 'ɒ': 17, 'ʌ': 18, 'z': 19, 'm': 20, 'iː': 21, 'aɪ': 22, 'h': 23, 'e': 24, 'kʰ': 25, 's': 26, 'ə': 27, 'ɔː': 28, 'ɹ': 29, 'i': 30, 'əʊ': 31, 'uː': 32, 'j': 33, 'ɪə': 34, 'ɔɪ': 35, 'v': 36, 'f': 37, 'ɜː': 38, 'b': 39, 'pʰ': 40, 'd̠ʒ': 41, 'ɐ': 42, 'eə': 43, 'ʃ': 44, 'θ': 45, 'ŋ': 46, 'aʊ': 47, 'ʊə': 48, 'n̩': 49, 'ʒ': 50}\n",
      "Vocab size:  51\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ð': 4, 'æ': 5, 'tʰ': 6, 'ɡ': 7, 'ʊ': 8, 'd': 9, 'ɑː': 10, 'l': 11, 'ɪ': 12, 'n': 13, 'eɪ': 14, 't̠ʃ': 15, 'w': 16, 'ɒ': 17, 'ʌ': 18, 'z': 19, 'm': 20, 'iː': 21, 'aɪ': 22, 'h': 23, 'e': 24, 'kʰ': 25, 's': 26, 'ə': 27, 'ɔː': 28, 'ɹ': 29, 'i': 30, 'əʊ': 31, 'uː': 32, 'j': 33, 'ɪə': 34, 'ɔɪ': 35, 'v': 36, 'f': 37, 'ɜː': 38, 'b': 39, 'pʰ': 40, 'd̠ʒ': 41, 'ɐ': 42, 'eə': 43, 'ʃ': 44, 'θ': 45, 'ŋ': 46, 'aʊ': 47, 'ʊə': 48, 'n̩': 49, 'ʒ': 50}\n",
      "New vocab size:  51\n",
      "Tokenizer for EnglishUK saved.\n",
      "\n",
      "Training tokenizer for French...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a': 5, 'ɑ̃': 6, 'd': 7, 'ɔ': 8, 'n': 9, 'b': 10, 'ʁ': 11, 'ə': 12, 'ɡ': 13, 'ʒ': 14, 'i': 15, 'v': 16, 't': 17, 'k': 18, 'o': 19, 'ɛ̃': 20, 'w': 21, 'y': 22, 'j': 23, 'e': 24, 'ɔ̃': 25, 'p': 26, 'ɛ': 27, 'f': 28, 's': 29, 'z': 30, 'l': 31, 'u': 32, 'ʃ': 33, 'œ': 34, 'ø': 35, 'ɲ': 36, 't̠ʃ': 37, 'd̠ʒ': 38}\n",
      "Vocab size:  39\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a': 5, 'ɑ̃': 6, 'd': 7, 'ɔ': 8, 'n': 9, 'b': 10, 'ʁ': 11, 'ə': 12, 'ɡ': 13, 'ʒ': 14, 'i': 15, 'v': 16, 't': 17, 'k': 18, 'o': 19, 'ɛ̃': 20, 'w': 21, 'y': 22, 'j': 23, 'e': 24, 'ɔ̃': 25, 'p': 26, 'ɛ': 27, 'f': 28, 's': 29, 'z': 30, 'l': 31, 'u': 32, 'ʃ': 33, 'œ': 34, 'ø': 35, 'ɲ': 36, 't̠ʃ': 37, 'd̠ʒ': 38}\n",
      "New vocab size:  39\n",
      "Tokenizer for French saved.\n",
      "\n",
      "Training tokenizer for German...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'aː': 4, 'oː': 5, 'a': 6, 'b': 7, 'x': 8, 'v': 9, 'øː': 10, 'n': 11, 'ɛː': 12, 'f': 13, 'l': 14, 'iː': 15, 'yː': 16, 'j': 17, 'uː': 18, 'h': 19, 'ʊ': 20, 'm': 21, 'ɔ': 22, 'ɪ': 23, 'eː': 24, 'ə': 25, 'd̺': 26, 't̺ʰ': 27, 'ɛ': 28, 'ŋ': 29, 'ç': 30, 'œ': 31, 'kʰ': 32, 'ʀ': 33, 'ɡ': 34, 'pʰ': 35, 'ʏ': 36, 's': 37, 'z': 38, 'ts': 39, 'ʃ': 40, 'ɐ': 41, 'pf': 42, 't̠ʃ': 43, 'd̠ʒ': 44}\n",
      "Vocab size:  45\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'aː': 4, 'oː': 5, 'a': 6, 'b': 7, 'x': 8, 'v': 9, 'øː': 10, 'n': 11, 'ɛː': 12, 'f': 13, 'l': 14, 'iː': 15, 'yː': 16, 'j': 17, 'uː': 18, 'h': 19, 'ʊ': 20, 'm': 21, 'ɔ': 22, 'ɪ': 23, 'eː': 24, 'ə': 25, 'd̺': 26, 't̺ʰ': 27, 'ɛ': 28, 'ŋ': 29, 'ç': 30, 'œ': 31, 'kʰ': 32, 'ʀ': 33, 'ɡ': 34, 'pʰ': 35, 'ʏ': 36, 's': 37, 'z': 38, 'ts': 39, 'ʃ': 40, 'ɐ': 41, 'pf': 42, 't̠ʃ': 43, 'd̠ʒ': 44}\n",
      "New vocab size:  45\n",
      "Tokenizer for German saved.\n",
      "\n",
      "Training tokenizer for Spanish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'i': 5, 'ɾ': 6, 'e̞': 7, 'n': 8, 'k': 9, 'ɲ': 10, 'o̞': 11, 'm': 12, 's': 13, 'u': 14, 'p': 15, 'd': 16, 'l': 17, 't': 18, 'β': 19, 'ɡ': 20, 'w': 21, 'ʝ': 22, 'f': 23, 'x': 24, 'j': 25, 'r': 26, 't̠ʃ': 27, 'ʃ': 28, 'tl': 29, 'ts': 30}\n",
      "Vocab size:  31\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'i': 5, 'ɾ': 6, 'e̞': 7, 'n': 8, 'k': 9, 'ɲ': 10, 'o̞': 11, 'm': 12, 's': 13, 'u': 14, 'p': 15, 'd': 16, 'l': 17, 't': 18, 'β': 19, 'ɡ': 20, 'w': 21, 'ʝ': 22, 'f': 23, 'x': 24, 'j': 25, 'r': 26, 't̠ʃ': 27, 'ʃ': 28, 'tl': 29, 'ts': 30}\n",
      "New vocab size:  31\n",
      "Tokenizer for Spanish saved.\n",
      "\n",
      "Training tokenizer for Dutch...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'z': 4, 'oː': 5, 'j': 6, 'ãː': 7, 'ɦ': 8, 'ɾ': 9, 'd': 10, 'i': 11, 'ɛ': 12, 'p': 13, 'ɪ': 14, 'k': 15, 'ɑ': 16, 'l': 17, 'ɛː': 18, 'n': 19, 's': 20, 'v': 21, 'ə': 22, 'ɛi': 23, 'ʋ': 24, 't': 25, 'm': 26, 'ɣ': 27, 'ʏ': 28, 'ɔ': 29, 'x': 30, 'u': 31, 'f': 32, 'ŋ': 33, 'øː': 34, 'b': 35, 'ɔː': 36, 'ʌu': 37, 'y': 38, 'œy': 39, 'tʲ': 40, 'w': 41, 'ʃ': 42, 't̠ʃ': 43, 'ɲ': 44, 'ʒ': 45, 'iː': 46, 'ɡ': 47, 'd̠ʒ': 48, 'ã': 49}\n",
      "Vocab size:  50\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'z': 4, 'oː': 5, 'j': 6, 'ãː': 7, 'ɦ': 8, 'ɾ': 9, 'd': 10, 'i': 11, 'ɛ': 12, 'p': 13, 'ɪ': 14, 'k': 15, 'ɑ': 16, 'l': 17, 'ɛː': 18, 'n': 19, 's': 20, 'v': 21, 'ə': 22, 'ɛi': 23, 'ʋ': 24, 't': 25, 'm': 26, 'ɣ': 27, 'ʏ': 28, 'ɔ': 29, 'x': 30, 'u': 31, 'f': 32, 'ŋ': 33, 'øː': 34, 'b': 35, 'ɔː': 36, 'ʌu': 37, 'y': 38, 'œy': 39, 'tʲ': 40, 'w': 41, 'ʃ': 42, 't̠ʃ': 43, 'ɲ': 44, 'ʒ': 45, 'iː': 46, 'ɡ': 47, 'd̠ʒ': 48, 'ã': 49}\n",
      "New vocab size:  50\n",
      "Tokenizer for Dutch saved.\n",
      "\n",
      "Training tokenizer for Mandarin...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a˧˥': 4, 'u˧˥': 5, 'a˥': 6, 'au': 7, 'n': 8, 'a˥˩': 9, 'ʃ̺': 10, 'ɻ̩˥˩': 11, 'ə˧˥': 12, 'm': 13, 'ɤ': 14, 'p': 15, 'j': 16, 'e˧˥': 17, 'kʰ': 18, 'k': 19, 'ɤ˥˩': 20, 'w': 21, 'o˥': 22, 't̠ʃ̺ʰ': 23, 'ə˥': 24, 'ŋ': 25, 't': 26, 'ʊ˥': 27, 'ɕ': 28, 'i': 29, 'a': 30, 'l': 31, 'au˧˩˧': 32, 'x': 33, 'u˧˩˧': 34, 'i˥': 35, 'ei˧˩˧': 36, 'pʰ': 37, 'i˧˥': 38, 'ai˧˥': 39, 'ou˧˩˧': 40, 'ɤ˧˥': 41, 'o˧˩˧': 42, 'tɕ': 43, 'au˥˩': 44, 'ts': 45, 'ə˧˩˧': 46, 'ɤ˥': 47, 'ei˧˥': 48, 'ʊ˧˥': 49, 'i˧˩˧': 50, 't̠ʃ̺': 51, 'ɻ̩˧˩˧': 52, 'ei˥˩': 53, 's': 54, 'u˥˩': 55, 'ɹ̪̩': 56, 'ai˥': 57, 'u˥': 58, 'tɕʰ': 59, 'a˧˩˧': 60, 'ai˥˩': 61, 'ɛ˥˩': 62, 'f': 63, 'i˥˩': 64, 'y˥˩': 65, 'au˧˥': 66, 'ɻ': 67, 'ou˥˩': 68, 'e˥': 69, 'tʰ': 70, 'ɹ̪̩˥˩': 71, 'ɛ˧˥': 72, 'au˥': 73, 'ou˧˥': 74, 'e˧˩˧': 75, 'ɛ˥': 76, 'ɻ̩˥': 77, 'ɥ': 78, 'ɹ̪̩˧˩˧': 79, 'ai˧˩˧': 80, 'ou˥': 81, 'o˥˩': 82, 'ɛ˧˩˧': 83, 'ʊ˧˩˧': 84, 'ɔ˥': 85, 'tsʰ': 86, 'ei': 87, 'ə˥˩': 88, 'o': 89, 'ʊ˥˩': 90, 'ou': 91, 'ɤ˧˩˧': 92, 'o˧˥': 93, 'ei˥': 94, 'e˥˩': 95, 'ɚ˧˩˧': 96, 'y˥': 97, 'ɚ˥˩': 98, 'y˧˥': 99, 'ɻ̩': 100, 'y˧˩˧': 101, 'ɹ̪̩˥': 102, 'ɻ̩˧˥': 103, 'u': 104, 'ə': 105, 'ai': 106, 'ʊ': 107, 'e': 108, 'ɚ˧˥': 109, 'ɔ˥˩': 110, 'ɹ̪̩˧˥': 111, 'ɛ': 112, 'y': 113, 'm˧˥': 114}\n",
      "Vocab size:  115\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a˧˥': 4, 'u˧˥': 5, 'a˥': 6, 'au': 7, 'n': 8, 'a˥˩': 9, 'ʃ̺': 10, 'ɻ̩˥˩': 11, 'ə˧˥': 12, 'm': 13, 'ɤ': 14, 'p': 15, 'j': 16, 'e˧˥': 17, 'kʰ': 18, 'k': 19, 'ɤ˥˩': 20, 'w': 21, 'o˥': 22, 't̠ʃ̺ʰ': 23, 'ə˥': 24, 'ŋ': 25, 't': 26, 'ʊ˥': 27, 'ɕ': 28, 'i': 29, 'a': 30, 'l': 31, 'au˧˩˧': 32, 'x': 33, 'u˧˩˧': 34, 'i˥': 35, 'ei˧˩˧': 36, 'pʰ': 37, 'i˧˥': 38, 'ai˧˥': 39, 'ou˧˩˧': 40, 'ɤ˧˥': 41, 'o˧˩˧': 42, 'tɕ': 43, 'au˥˩': 44, 'ts': 45, 'ə˧˩˧': 46, 'ɤ˥': 47, 'ei˧˥': 48, 'ʊ˧˥': 49, 'i˧˩˧': 50, 't̠ʃ̺': 51, 'ɻ̩˧˩˧': 52, 'ei˥˩': 53, 's': 54, 'u˥˩': 55, 'ɹ̪̩': 56, 'ai˥': 57, 'u˥': 58, 'tɕʰ': 59, 'a˧˩˧': 60, 'ai˥˩': 61, 'ɛ˥˩': 62, 'f': 63, 'i˥˩': 64, 'y˥˩': 65, 'au˧˥': 66, 'ɻ': 67, 'ou˥˩': 68, 'e˥': 69, 'tʰ': 70, 'ɹ̪̩˥˩': 71, 'ɛ˧˥': 72, 'au˥': 73, 'ou˧˥': 74, 'e˧˩˧': 75, 'ɛ˥': 76, 'ɻ̩˥': 77, 'ɥ': 78, 'ɹ̪̩˧˩˧': 79, 'ai˧˩˧': 80, 'ou˥': 81, 'o˥˩': 82, 'ɛ˧˩˧': 83, 'ʊ˧˩˧': 84, 'ɔ˥': 85, 'tsʰ': 86, 'ei': 87, 'ə˥˩': 88, 'o': 89, 'ʊ˥˩': 90, 'ou': 91, 'ɤ˧˩˧': 92, 'o˧˥': 93, 'ei˥': 94, 'e˥˩': 95, 'ɚ˧˩˧': 96, 'y˥': 97, 'ɚ˥˩': 98, 'y˧˥': 99, 'ɻ̩': 100, 'y˧˩˧': 101, 'ɹ̪̩˥': 102, 'ɻ̩˧˥': 103, 'u': 104, 'ə': 105, 'ai': 106, 'ʊ': 107, 'e': 108, 'ɚ˧˥': 109, 'ɔ˥˩': 110, 'ɹ̪̩˧˥': 111, 'ɛ': 112, 'y': 113, 'm˧˥': 114}\n",
      "New vocab size:  115\n",
      "Tokenizer for Mandarin saved.\n",
      "\n",
      "Training tokenizer for Japanese...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'kʲ': 4, 'aː': 5, 'o': 6, 'ts': 7, 'ɯ': 8, 'k': 9, 'a': 10, 'i': 11, 'w': 12, 'd̠ʒ': 13, 't': 14, 'e': 15, 'n': 16, 'ʃ': 17, 'd': 18, 'b': 19, 's': 20, 'm': 21, 'h': 22, 'ɾ': 23, 't̠ʃ': 24, 'ɯː': 25, 'p': 26, 'j': 27, 'ɡʲ': 28, 'ɸ': 29, 'ɡ': 30, 'oː': 31, 'ɲ': 32, 'z': 33, 'eː': 34, 'pʲ': 35, 'ɾʲ': 36, 'ç': 37, 'bʲ': 38, 'mʲ': 39}\n",
      "Vocab size:  40\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'kʲ': 4, 'aː': 5, 'o': 6, 'ts': 7, 'ɯ': 8, 'k': 9, 'a': 10, 'i': 11, 'w': 12, 'd̠ʒ': 13, 't': 14, 'e': 15, 'n': 16, 'ʃ': 17, 'd': 18, 'b': 19, 's': 20, 'm': 21, 'h': 22, 'ɾ': 23, 't̠ʃ': 24, 'ɯː': 25, 'p': 26, 'j': 27, 'ɡʲ': 28, 'ɸ': 29, 'ɡ': 30, 'oː': 31, 'ɲ': 32, 'z': 33, 'eː': 34, 'pʲ': 35, 'ɾʲ': 36, 'ç': 37, 'bʲ': 38, 'mʲ': 39}\n",
      "New vocab size:  40\n",
      "Tokenizer for Japanese saved.\n",
      "\n",
      "Training tokenizer for Cantonese...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'j': 4, 'ɐ˥': 5, 't': 6, 'k': 7, 'ɐu˧˥': 8, 'i˨': 9, 'n': 10, 'i˧˩̰': 11, 'y˨': 12, 's': 13, 'ɐ˨': 14, 'p': 15, 'ts': 16, 'ɐu˥': 17, 'ɪ̞˧˥': 18, 'ŋ': 19, 'ɵ˧': 20, 'a̞˧': 21, 'l': 22, 'ʊ̟˥': 23, 'a̞˧˩̰': 24, 'ɛ˥': 25, 'ei˩˧': 26, 'w': 27, 'a̞˨': 28, 'ɐi˧˥': 29, 'a̞˧˥': 30, 'm̩˧˥': 31, 'm': 32, 'ou˥': 33, 'ei˥': 34, 'i˧': 35, 'ɔ̽˧˥': 36, 'tʰ': 37, 'i˥': 38, 'f': 39, 'aːĭ˧': 40, 'h': 41, 'ɵy˧': 42, 'a̞˥': 43, 'ei˧˩̰': 44, 'ou˨': 45, 'ɔ̽˧': 46, 'ɐi˧˩̰': 47, 'u˧': 48, 'ɔːĭ˥': 49, 'ɐu˨': 50, 'ei˧˥': 51, 'ɐi˨': 52, 'ʊ̟˧˩̰': 53, 'ʊ̟˨': 54, 'a̞˩˧': 55, 'ou˧˥': 56, 'aːĭ˧˥': 57, 'ɔ̽˨': 58, 'ɛ˩˧': 59, 'ɪ̞˨': 60, 'iːŭ˧': 61, 'ɛ˧˩̰': 62, 'm̩˧˩̰': 63, 'ɵ˧˥': 64, 'ei˧': 65, 'ɐu˧˩̰': 66, 'm̩˧': 67, 'ɐ˧˥': 68, 'ɐu˩˧': 69, 'ɐi˥': 70, 'ɔ̽˥': 71, 'ɔ̽˧˩̰': 72, 'ɔːĭ˧': 73, 'ou˩˧': 74, 'm̩˥': 75, 'ɐ˧': 76, 'tsʰ': 77, 'ɛ˧˥': 78, 'i˧˥': 79, 'ɔ̽˩˧': 80, 'kʰ': 81, 'ɐ˧˩̰': 82, 'aːŭ˧˥': 83, 'pʰ': 84, 'aːĭ˧˩̰': 85, 'ɵy˩˧': 86, 'ɛ˧': 87, 'u˧˥': 88, 'ɛ˨': 89, 'ʊ̟˧': 90, 'u˥': 91, 'm̩˩˧': 92, 'aːŭ˧': 93, 'œ̞˩˧': 94, 'i˩˧': 95, 'ɪ̞˧˩̰': 96, 'u˨': 97, 'ɪ̞˥': 98, 'iːŭ˧˩̰': 99, 'œ̞˧˥': 100, 'y˧': 101, 'uːĭ˩˧': 102, 'uːĭ˥': 103, 'ɵy˧˥': 104, 'y˧˩̰': 105, 'ɔːĭ˧˥': 106, 'ɛ': 107, 'ou˧': 108, 'ei˨': 109, 'ɵ˥': 110, 'u˧˩̰': 111, 'y˥': 112, 'œ̞˥': 113, 'œ̞˧˩̰': 114, 'aːĭ˨': 115, 'ɐ˩˧': 116, 'œ̞˧': 117, 'uːĭ˧˥': 118, 'ɐu˧': 119, 'ɐi˩˧': 120, 'ɐi˧': 121, 'ou˧˩̰': 122, 'aːĭ˥': 123, 'aːŭ˥': 124, 'ŋ˩˧': 125, 'y˧˥': 126, 'iːŭ˥': 127, 'ɔːĭ˨': 128, 'ʊ̟˧˥': 129, 'iːŭ˧˥': 130, 'ɵy˥': 131, 'ɔːĭ˧˩̰': 132, 'uːĭ˧': 133, 'ɵy˧˩̰': 134, 'œ̞˨': 135, 'm̩˨': 136, 'aːŭ˧˩̰': 137, 'y˩˧': 138, 'aːŭ˩˧': 139, 'aːĭ˩˧': 140, 'uːĭ˨': 141, 'ɵy˨': 142, 'aːŭ˨': 143, 'ɪ̞˧': 144, 'ɵ˨': 145, 'iːŭ˩˧': 146, 'iːŭ˨': 147, 'ɵ˧˩̰': 148, 'uːĭ˧˩̰': 149, 'u˩˧': 150, 'ŋ˧˩̰': 151}\n",
      "Vocab size:  152\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'j': 4, 'ɐ˥': 5, 't': 6, 'k': 7, 'ɐu˧˥': 8, 'i˨': 9, 'n': 10, 'i˧˩̰': 11, 'y˨': 12, 's': 13, 'ɐ˨': 14, 'p': 15, 'ts': 16, 'ɐu˥': 17, 'ɪ̞˧˥': 18, 'ŋ': 19, 'ɵ˧': 20, 'a̞˧': 21, 'l': 22, 'ʊ̟˥': 23, 'a̞˧˩̰': 24, 'ɛ˥': 25, 'ei˩˧': 26, 'w': 27, 'a̞˨': 28, 'ɐi˧˥': 29, 'a̞˧˥': 30, 'm̩˧˥': 31, 'm': 32, 'ou˥': 33, 'ei˥': 34, 'i˧': 35, 'ɔ̽˧˥': 36, 'tʰ': 37, 'i˥': 38, 'f': 39, 'aːĭ˧': 40, 'h': 41, 'ɵy˧': 42, 'a̞˥': 43, 'ei˧˩̰': 44, 'ou˨': 45, 'ɔ̽˧': 46, 'ɐi˧˩̰': 47, 'u˧': 48, 'ɔːĭ˥': 49, 'ɐu˨': 50, 'ei˧˥': 51, 'ɐi˨': 52, 'ʊ̟˧˩̰': 53, 'ʊ̟˨': 54, 'a̞˩˧': 55, 'ou˧˥': 56, 'aːĭ˧˥': 57, 'ɔ̽˨': 58, 'ɛ˩˧': 59, 'ɪ̞˨': 60, 'iːŭ˧': 61, 'ɛ˧˩̰': 62, 'm̩˧˩̰': 63, 'ɵ˧˥': 64, 'ei˧': 65, 'ɐu˧˩̰': 66, 'm̩˧': 67, 'ɐ˧˥': 68, 'ɐu˩˧': 69, 'ɐi˥': 70, 'ɔ̽˥': 71, 'ɔ̽˧˩̰': 72, 'ɔːĭ˧': 73, 'ou˩˧': 74, 'm̩˥': 75, 'ɐ˧': 76, 'tsʰ': 77, 'ɛ˧˥': 78, 'i˧˥': 79, 'ɔ̽˩˧': 80, 'kʰ': 81, 'ɐ˧˩̰': 82, 'aːŭ˧˥': 83, 'pʰ': 84, 'aːĭ˧˩̰': 85, 'ɵy˩˧': 86, 'ɛ˧': 87, 'u˧˥': 88, 'ɛ˨': 89, 'ʊ̟˧': 90, 'u˥': 91, 'm̩˩˧': 92, 'aːŭ˧': 93, 'œ̞˩˧': 94, 'i˩˧': 95, 'ɪ̞˧˩̰': 96, 'u˨': 97, 'ɪ̞˥': 98, 'iːŭ˧˩̰': 99, 'œ̞˧˥': 100, 'y˧': 101, 'uːĭ˩˧': 102, 'uːĭ˥': 103, 'ɵy˧˥': 104, 'y˧˩̰': 105, 'ɔːĭ˧˥': 106, 'ɛ': 107, 'ou˧': 108, 'ei˨': 109, 'ɵ˥': 110, 'u˧˩̰': 111, 'y˥': 112, 'œ̞˥': 113, 'œ̞˧˩̰': 114, 'aːĭ˨': 115, 'ɐ˩˧': 116, 'œ̞˧': 117, 'uːĭ˧˥': 118, 'ɐu˧': 119, 'ɐi˩˧': 120, 'ɐi˧': 121, 'ou˧˩̰': 122, 'aːĭ˥': 123, 'aːŭ˥': 124, 'ŋ˩˧': 125, 'y˧˥': 126, 'iːŭ˥': 127, 'ɔːĭ˨': 128, 'ʊ̟˧˥': 129, 'iːŭ˧˥': 130, 'ɵy˥': 131, 'ɔːĭ˧˩̰': 132, 'uːĭ˧': 133, 'ɵy˧˩̰': 134, 'œ̞˨': 135, 'm̩˨': 136, 'aːŭ˧˩̰': 137, 'y˩˧': 138, 'aːŭ˩˧': 139, 'aːĭ˩˧': 140, 'uːĭ˨': 141, 'ɵy˨': 142, 'aːŭ˨': 143, 'ɪ̞˧': 144, 'ɵ˨': 145, 'iːŭ˩˧': 146, 'iːŭ˨': 147, 'ɵ˧˩̰': 148, 'uːĭ˧˩̰': 149, 'u˩˧': 150, 'ŋ˧˩̰': 151}\n",
      "New vocab size:  152\n",
      "Tokenizer for Cantonese saved.\n",
      "\n",
      "Training tokenizer for Estonian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'n': 4, 'o': 5, 't': 6, 'ʃ': 7, 'a': 8, 'uː': 9, 'm': 10, 'u': 11, 'tʲ': 12, 'i': 13, 's': 14, 'eː': 15, 'd': 16, 'iː': 17, 'k': 18, 'ɡ': 19, 'ɑ': 20, 'ɤ': 21, 'ʊ': 22, 'sʲ': 23, 'j': 24, 'aː': 25, 'h': 26, 'v': 27, 'æi': 28, 'kː': 29, 'e': 30, 'ɪ': 31, 'tː': 32, 'r': 33, 'ɛ': 34, 'mː': 35, 'p': 36, 'sː': 37, 'æ': 38, 'l': 39, 'pː': 40, 'yː': 41, 'æː': 42, 'b': 43, 'ɔ': 44, 'ɤː': 45, 'lː': 46, 'ø': 47, 'øː': 48, 'ŋ': 49, 'y': 50, 'oː': 51, 'rː': 52, 'ɲ': 53, 'nː': 54, 'w': 55, 'tʲː': 56, 'øɪ̯': 57, 'f': 58, 'dʲ': 59, 'sʲː': 60, 't̠ʃ': 61, 'ʃː': 62, 'ʒ': 63, 'z': 64, 'fː': 65, 'dː': 66, 'yi': 67}\n",
      "Vocab size:  68\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'n': 4, 'o': 5, 't': 6, 'ʃ': 7, 'a': 8, 'uː': 9, 'm': 10, 'u': 11, 'tʲ': 12, 'i': 13, 's': 14, 'eː': 15, 'd': 16, 'iː': 17, 'k': 18, 'ɡ': 19, 'ɑ': 20, 'ɤ': 21, 'ʊ': 22, 'sʲ': 23, 'j': 24, 'aː': 25, 'h': 26, 'v': 27, 'æi': 28, 'kː': 29, 'e': 30, 'ɪ': 31, 'tː': 32, 'r': 33, 'ɛ': 34, 'mː': 35, 'p': 36, 'sː': 37, 'æ': 38, 'l': 39, 'pː': 40, 'yː': 41, 'æː': 42, 'b': 43, 'ɔ': 44, 'ɤː': 45, 'lː': 46, 'ø': 47, 'øː': 48, 'ŋ': 49, 'y': 50, 'oː': 51, 'rː': 52, 'ɲ': 53, 'nː': 54, 'w': 55, 'tʲː': 56, 'øɪ̯': 57, 'f': 58, 'dʲ': 59, 'sʲː': 60, 't̠ʃ': 61, 'ʃː': 62, 'ʒ': 63, 'z': 64, 'fː': 65, 'dː': 66, 'yi': 67}\n",
      "New vocab size:  68\n",
      "Tokenizer for Estonian saved.\n",
      "\n",
      "Training tokenizer for Croatian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'e': 4, 'a': 5, 'u': 6, 'x': 7, 'k': 8, 't̪': 9, 'n': 10, 'o': 11, 'd̪': 12, 'i': 13, 'r': 14, 'm': 15, 'ʃ': 16, 'p': 17, 's': 18, 'ʋ': 19, 'j': 20, 't̠ʃ': 21, 'l': 22, 'ɡ': 23, 'ʒ': 24, 'b': 25, 't̪s': 26, 'z': 27, 'd̠ʒ': 28, 'ʎ': 29, 'f': 30, 'ɲ': 31, 'y': 32, 'q': 33, 'w': 34}\n",
      "Vocab size:  35\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'e': 4, 'a': 5, 'u': 6, 'x': 7, 'k': 8, 't̪': 9, 'n': 10, 'o': 11, 'd̪': 12, 'i': 13, 'r': 14, 'm': 15, 'ʃ': 16, 'p': 17, 's': 18, 'ʋ': 19, 'j': 20, 't̠ʃ': 21, 'l': 22, 'ɡ': 23, 'ʒ': 24, 'b': 25, 't̪s': 26, 'z': 27, 'd̠ʒ': 28, 'ʎ': 29, 'f': 30, 'ɲ': 31, 'y': 32, 'q': 33, 'w': 34}\n",
      "New vocab size:  35\n",
      "Tokenizer for Croatian saved.\n",
      "\n",
      "Training tokenizer for Danish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'n': 4, 'oˤ': 5, 't': 6, 'y': 7, 'ə': 8, 'ð': 9, 'ʁ': 10, 'ɑˤː': 11, 's': 12, 'k': 13, 'i': 14, 'b': 15, 'eˤ': 16, 't̠ʃ': 17, 'a': 18, 'l': 19, 'd': 20, 'ɡ': 21, 'f': 22, 'e': 23, 'ɛ': 24, 'r': 25, 'ɔ': 26, 'w': 27, 'ɔˤ': 28, 'm': 29, 'uˤ': 30, 'j': 31, 'ɑ': 32, 'u': 33, 'ɒ': 34, 'iˤ': 35, 'ʋ': 36, 'h': 37, 'œ': 38, 'p': 39, 'ɕ': 40, 'o': 41, 'ŋ': 42, 'ɒː': 43, 'aˤ': 44, 'ɜ': 45, 'œː': 46, 'eː': 47, 'aː': 48, 'd̠ʒ': 49, 'uː': 50, 'ɔː': 51, 'oː': 52, 'iː': 53, 'yː': 54}\n",
      "Vocab size:  55\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'n': 4, 'oˤ': 5, 't': 6, 'y': 7, 'ə': 8, 'ð': 9, 'ʁ': 10, 'ɑˤː': 11, 's': 12, 'k': 13, 'i': 14, 'b': 15, 'eˤ': 16, 't̠ʃ': 17, 'a': 18, 'l': 19, 'd': 20, 'ɡ': 21, 'f': 22, 'e': 23, 'ɛ': 24, 'r': 25, 'ɔ': 26, 'w': 27, 'ɔˤ': 28, 'm': 29, 'uˤ': 30, 'j': 31, 'ɑ': 32, 'u': 33, 'ɒ': 34, 'iˤ': 35, 'ʋ': 36, 'h': 37, 'œ': 38, 'p': 39, 'ɕ': 40, 'o': 41, 'ŋ': 42, 'ɒː': 43, 'aˤ': 44, 'ɜ': 45, 'œː': 46, 'eː': 47, 'aː': 48, 'd̠ʒ': 49, 'uː': 50, 'ɔː': 51, 'oː': 52, 'iː': 53, 'yː': 54}\n",
      "New vocab size:  55\n",
      "Tokenizer for Danish saved.\n",
      "\n",
      "Training tokenizer for Basque...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'b': 4, 'ai̯': 5, 'e': 6, 's̪̻': 7, 'ɟ': 8, 'ei̯': 9, 't̺s̺': 10, 'i': 11, 'oi̯': 12, 'a': 13, 'ɾ': 14, 'k': 15, 't̠ʃ': 16, 's̺': 17, 'l': 18, 'p': 19, 'o': 20, 'r': 21, 't̪': 22, 'u': 23, 'n': 24, 'm': 25, 'ð': 26, 't̪̻s̪̻': 27, 'β': 28, 'ʎ': 29, 'ɡ': 30, 'ɣ': 31, 'au̯': 32, 'c': 33, 'j': 34, 'd̪': 35, 'ʃ': 36, 'ɲ': 37, 'f': 38, 'eu̯': 39, 'θ': 40, 'x': 41}\n",
      "Vocab size:  42\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'b': 4, 'ai̯': 5, 'e': 6, 's̪̻': 7, 'ɟ': 8, 'ei̯': 9, 't̺s̺': 10, 'i': 11, 'oi̯': 12, 'a': 13, 'ɾ': 14, 'k': 15, 't̠ʃ': 16, 's̺': 17, 'l': 18, 'p': 19, 'o': 20, 'r': 21, 't̪': 22, 'u': 23, 'n': 24, 'm': 25, 'ð': 26, 't̪̻s̪̻': 27, 'β': 28, 'ʎ': 29, 'ɡ': 30, 'ɣ': 31, 'au̯': 32, 'c': 33, 'j': 34, 'd̪': 35, 'ʃ': 36, 'ɲ': 37, 'f': 38, 'eu̯': 39, 'θ': 40, 'x': 41}\n",
      "New vocab size:  42\n",
      "Tokenizer for Basque saved.\n",
      "\n",
      "Training tokenizer for Hungarian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'i': 4, 'd̪': 5, 'ɛ': 6, 'b': 7, 'aː': 8, 't̠ʃ': 9, 'm': 10, 'l̪': 11, 's̻': 12, 'z̻': 13, 'ɡ': 14, 'k': 15, 'o': 16, 'ɑ': 17, 't̪ː': 18, 'j': 19, 'ø': 20, 'n̪': 21, 'ɲ': 22, 'u': 23, 't̻s̻': 24, 'y': 25, 'r̪': 26, 'h': 27, 'oː': 28, 'v': 29, 'd̠ʒ': 30, 't̪': 31, 'eː': 32, 'ʃ': 33, 'ɟʝ': 34, 's̻ː': 35, 'p': 36, 'øː': 37, 'mː': 38, 'z̻ː': 39, 'l̪ː': 40, 'f': 41, 'ɟʝː': 42, 'uː': 43, 'n̪ː': 44, 'iː': 45, 'ɲː': 46, 'ʃː': 47, 'r̪ː': 48, 'kː': 49, 'ŋ': 50, 't̠ʃː': 51, 'jː': 52, 'bː': 53, 'cç': 54, 't̻s̻ː': 55, 'd̪ː': 56, 'ɡː': 57, 'pː': 58, 'ʒ': 59, 'vː': 60, 'cçː': 61, 'fː': 62, 'hː': 63, 'yː': 64}\n",
      "Vocab size:  65\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'i': 4, 'd̪': 5, 'ɛ': 6, 'b': 7, 'aː': 8, 't̠ʃ': 9, 'm': 10, 'l̪': 11, 's̻': 12, 'z̻': 13, 'ɡ': 14, 'k': 15, 'o': 16, 'ɑ': 17, 't̪ː': 18, 'j': 19, 'ø': 20, 'n̪': 21, 'ɲ': 22, 'u': 23, 't̻s̻': 24, 'y': 25, 'r̪': 26, 'h': 27, 'oː': 28, 'v': 29, 'd̠ʒ': 30, 't̪': 31, 'eː': 32, 'ʃ': 33, 'ɟʝ': 34, 's̻ː': 35, 'p': 36, 'øː': 37, 'mː': 38, 'z̻ː': 39, 'l̪ː': 40, 'f': 41, 'ɟʝː': 42, 'uː': 43, 'n̪ː': 44, 'iː': 45, 'ɲː': 46, 'ʃː': 47, 'r̪ː': 48, 'kː': 49, 'ŋ': 50, 't̠ʃː': 51, 'jː': 52, 'bː': 53, 'cç': 54, 't̻s̻ː': 55, 'd̪ː': 56, 'ɡː': 57, 'pː': 58, 'ʒ': 59, 'vː': 60, 'cçː': 61, 'fː': 62, 'hː': 63, 'yː': 64}\n",
      "New vocab size:  65\n",
      "Tokenizer for Hungarian saved.\n",
      "\n",
      "Training tokenizer for Turkish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'm': 5, 'h': 6, 'e': 7, 'ɾ': 8, 'k': 9, 'lʲ': 10, 'iː': 11, 'b': 12, 'f': 13, 'l̪ˠ': 14, 'n̪': 15, 'ɯ': 16, 'j': 17, 'o': 18, 'z̪': 19, 's̪': 20, 'v': 21, 'd̪': 22, 'i': 23, 'p': 24, 'ɟ': 25, 'œ': 26, 'y': 27, 'eː': 28, 'd̠ʒ': 29, 'ʃ': 30, 'u': 31, 'ɡ': 32, 't̪': 33, 't̠ʃ': 34, 'aː': 35, 'pː': 36, 'ʒ': 37, 'uː': 38, 'c': 39, 'w': 40}\n",
      "Vocab size:  41\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'm': 5, 'h': 6, 'e': 7, 'ɾ': 8, 'k': 9, 'lʲ': 10, 'iː': 11, 'b': 12, 'f': 13, 'l̪ˠ': 14, 'n̪': 15, 'ɯ': 16, 'j': 17, 'o': 18, 'z̪': 19, 's̪': 20, 'v': 21, 'd̪': 22, 'i': 23, 'p': 24, 'ɟ': 25, 'œ': 26, 'y': 27, 'eː': 28, 'd̠ʒ': 29, 'ʃ': 30, 'u': 31, 'ɡ': 32, 't̪': 33, 't̠ʃ': 34, 'aː': 35, 'pː': 36, 'ʒ': 37, 'uː': 38, 'c': 39, 'w': 40}\n",
      "New vocab size:  41\n",
      "Tokenizer for Turkish saved.\n",
      "\n",
      "Training tokenizer for Farsi...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a̟': 5, 'b': 6, 's': 7, 'e': 8, 'r': 9, 'j': 10, 'h': 11, 't̠ʃ': 12, 'kʰ': 13, 'd̪': 14, 'n̪': 15, 'z': 16, 'ʃ': 17, 'ɡ': 18, 'i': 19, 'u': 20, 'o': 21, 'f': 22, 't̪ʰ': 23, 'ɑ': 24, 'd̠ʒ': 25, 'v': 26, 'pʰ': 27, 'l': 28, 'w': 29, 'ɢ': 30}\n",
      "Vocab size:  31\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a̟': 5, 'b': 6, 's': 7, 'e': 8, 'r': 9, 'j': 10, 'h': 11, 't̠ʃ': 12, 'kʰ': 13, 'd̪': 14, 'n̪': 15, 'z': 16, 'ʃ': 17, 'ɡ': 18, 'i': 19, 'u': 20, 'o': 21, 'f': 22, 't̪ʰ': 23, 'ɑ': 24, 'd̠ʒ': 25, 'v': 26, 'pʰ': 27, 'l': 28, 'w': 29, 'ɢ': 30}\n",
      "New vocab size:  31\n",
      "Tokenizer for Farsi saved.\n",
      "\n",
      "Training tokenizer for Icelandic...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'pʰ': 4, 'iː': 5, 'i': 6, 'aː': 7, 'r̥': 8, 'ɪ': 9, 'ɛ': 10, 't̪ʰ': 11, 's̺': 12, 'j': 13, 'ä': 14, 'k': 15, 'ʋ': 16, 'ɛː': 17, 'r': 18, 'ei̯': 19, 'θ̻': 20, 'l': 21, 'n̪': 22, 't̪': 23, 'ɬ': 24, 'uː': 25, 'ð̺̞': 26, 'ɡ': 27, 'c': 28, 'h': 29, 'ɔ': 30, 'n̪̥': 31, 'äu̯': 32, 'ŋ̥': 33, 'ʏ': 34, 'm': 35, 'f': 36, 'ɔː': 37, 'x': 38, 'cʰ': 39, 'ou̯': 40, 'p': 41, 'ŋ': 42, 'øɪ̯': 43, 'äi̯': 44, 'ɰ': 45, 'ʏː': 46, 'u': 47, 'ɪː': 48, 'œ': 49, 'ç': 50, 'ə': 51, 'œː': 52, 'ɲ': 53, 'm̥': 54, 'ɔi̯': 55, 'z': 56, 'ɲ̥': 57}\n",
      "Vocab size:  58\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'pʰ': 4, 'iː': 5, 'i': 6, 'aː': 7, 'r̥': 8, 'ɪ': 9, 'ɛ': 10, 't̪ʰ': 11, 's̺': 12, 'j': 13, 'ä': 14, 'k': 15, 'ʋ': 16, 'ɛː': 17, 'r': 18, 'ei̯': 19, 'θ̻': 20, 'l': 21, 'n̪': 22, 't̪': 23, 'ɬ': 24, 'uː': 25, 'ð̺̞': 26, 'ɡ': 27, 'c': 28, 'h': 29, 'ɔ': 30, 'n̪̥': 31, 'äu̯': 32, 'ŋ̥': 33, 'ʏ': 34, 'm': 35, 'f': 36, 'ɔː': 37, 'x': 38, 'cʰ': 39, 'ou̯': 40, 'p': 41, 'ŋ': 42, 'øɪ̯': 43, 'äi̯': 44, 'ɰ': 45, 'ʏː': 46, 'u': 47, 'ɪː': 48, 'œ': 49, 'ç': 50, 'ə': 51, 'œː': 52, 'ɲ': 53, 'm̥': 54, 'ɔi̯': 55, 'z': 56, 'ɲ̥': 57}\n",
      "New vocab size:  58\n",
      "Tokenizer for Icelandic saved.\n",
      "\n",
      "Training tokenizer for Indonesian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 's': 4, 'i': 5, 'n': 6, 'm': 7, 'a': 8, 'j': 9, 'u': 10, 'k': 11, 'o': 12, 'h': 13, 'l': 14, 't': 15, 'w': 16, 'd̠ʒ': 17, 'ŋ': 18, 'ə': 19, 'd': 20, 'p': 21, 'ɡ': 22, 'b': 23, 'r': 24, 'ɲ': 25, 't̠ʃ': 26, 'f': 27, 'z': 28, 'ʃ': 29, 'x': 30}\n",
      "Vocab size:  31\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 's': 4, 'i': 5, 'n': 6, 'm': 7, 'a': 8, 'j': 9, 'u': 10, 'k': 11, 'o': 12, 'h': 13, 'l': 14, 't': 15, 'w': 16, 'd̠ʒ': 17, 'ŋ': 18, 'ə': 19, 'd': 20, 'p': 21, 'ɡ': 22, 'b': 23, 'r': 24, 'ɲ': 25, 't̠ʃ': 26, 'f': 27, 'z': 28, 'ʃ': 29, 'x': 30}\n",
      "New vocab size:  31\n",
      "Tokenizer for Indonesian saved.\n",
      "\n",
      "Training tokenizer for Irish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'kʰ': 4, 'a': 5, 'ɾ̪ʲ': 6, 'd̪ˠ': 7, 'eː': 8, 'ʃ': 9, 'ɪ': 10, 'n̪ˠ': 11, 'ə': 12, 'w': 13, 'l̪ˠ': 14, 'ɛ̝': 15, 'ɡ': 16, 'ɾ̪ˠ': 17, 'mˠ': 18, 'x': 19, 'iː': 20, 'sˠ': 21, 'bˠ': 22, 'pˠʰ': 23, 't̪ʲʰ': 24, 'ɔ̝': 25, 'cʰ': 26, 't̪ˠʰ': 27, 'h': 28, 'vˠ': 29, 'ʊ': 30, 'j': 31, 'oː': 32, 'ɑː': 33, 'fˠ': 34, 'd̠ʒ': 35, 'l̪ʲ': 36, 'iːə': 37, 'uːe': 38, 'uː': 39, 'n̪ʲ': 40, 'd̪ʲ': 41, 'ɐ': 42, 'mʲ': 43, 'pʲʰ': 44, 'ɣ': 45, 'ɐɪ': 46, 'ŋ': 47, 'i̞': 48, 'ç': 49, 'z': 50, 'fʲ': 51, 'ʒ': 52, 'bʲ': 53}\n",
      "Vocab size:  54\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'kʰ': 4, 'a': 5, 'ɾ̪ʲ': 6, 'd̪ˠ': 7, 'eː': 8, 'ʃ': 9, 'ɪ': 10, 'n̪ˠ': 11, 'ə': 12, 'w': 13, 'l̪ˠ': 14, 'ɛ̝': 15, 'ɡ': 16, 'ɾ̪ˠ': 17, 'mˠ': 18, 'x': 19, 'iː': 20, 'sˠ': 21, 'bˠ': 22, 'pˠʰ': 23, 't̪ʲʰ': 24, 'ɔ̝': 25, 'cʰ': 26, 't̪ˠʰ': 27, 'h': 28, 'vˠ': 29, 'ʊ': 30, 'j': 31, 'oː': 32, 'ɑː': 33, 'fˠ': 34, 'd̠ʒ': 35, 'l̪ʲ': 36, 'iːə': 37, 'uːe': 38, 'uː': 39, 'n̪ʲ': 40, 'd̪ʲ': 41, 'ɐ': 42, 'mʲ': 43, 'pʲʰ': 44, 'ɣ': 45, 'ɐɪ': 46, 'ŋ': 47, 'i̞': 48, 'ç': 49, 'z': 50, 'fʲ': 51, 'ʒ': 52, 'bʲ': 53}\n",
      "New vocab size:  54\n",
      "Tokenizer for Irish saved.\n",
      "\n",
      "Training tokenizer for Welsh...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'h': 5, 'm': 6, 'ai': 7, 'ɛ': 8, 'r': 9, 't': 10, 'ɑː': 11, 'p': 12, 'd': 13, 'iː': 14, 'b': 15, 'oː': 16, 'f': 17, 'eː': 18, 'χ': 19, 'w': 20, 'a': 21, 'n': 22, 'ø': 23, 'j': 24, 'au': 25, 'ə': 26, 'ɔi': 27, 'ð': 28, 'ɪ': 29, 's': 30, 'ɡ': 31, 'ʊi': 32, 'ʊ': 33, 'əi': 34, 'θ': 35, 'l': 36, 'ʌ': 37, 'ŋ': 38, 'v': 39, 'k': 40, 'ɬ': 41, 'ɪu': 42, 'uː': 43, 'ʃ': 44, 'ɛu': 45, 'd̠ʒ': 46, 'z': 47}\n",
      "Vocab size:  48\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'h': 5, 'm': 6, 'ai': 7, 'ɛ': 8, 'r': 9, 't': 10, 'ɑː': 11, 'p': 12, 'd': 13, 'iː': 14, 'b': 15, 'oː': 16, 'f': 17, 'eː': 18, 'χ': 19, 'w': 20, 'a': 21, 'n': 22, 'ø': 23, 'j': 24, 'au': 25, 'ə': 26, 'ɔi': 27, 'ð': 28, 'ɪ': 29, 's': 30, 'ɡ': 31, 'ʊi': 32, 'ʊ': 33, 'əi': 34, 'θ': 35, 'l': 36, 'ʌ': 37, 'ŋ': 38, 'v': 39, 'k': 40, 'ɬ': 41, 'ɪu': 42, 'uː': 43, 'ʃ': 44, 'ɛu': 45, 'd̠ʒ': 46, 'z': 47}\n",
      "New vocab size:  48\n",
      "Tokenizer for Welsh saved.\n",
      "\n",
      "Training tokenizer for Korean...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'i': 4, 'ɾ': 5, 'ɯ': 6, 'm': 7, 'a': 8, 'u': 9, 'j': 10, 'ɤ̞': 11, 'ɡ': 12, 'ŋ': 13, 'h': 14, 'æ': 15, 'p': 16, 'o': 17, 'dʑ': 18, 'w': 19, 'n̪': 20, 'd': 21, 'e': 22, 'l': 23, 't̠ʃ': 24, 'b': 25, 's̪': 26, 'k': 27, 't̪': 28, 'pʰ': 29, 'kʰ': 30, 'ɯi': 31, 't̠ʃʰ': 32}\n",
      "Vocab size:  33\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'i': 4, 'ɾ': 5, 'ɯ': 6, 'm': 7, 'a': 8, 'u': 9, 'j': 10, 'ɤ̞': 11, 'ɡ': 12, 'ŋ': 13, 'h': 14, 'æ': 15, 'p': 16, 'o': 17, 'dʑ': 18, 'w': 19, 'n̪': 20, 'd': 21, 'e': 22, 'l': 23, 't̠ʃ': 24, 'b': 25, 's̪': 26, 'k': 27, 't̪': 28, 'pʰ': 29, 'kʰ': 30, 'ɯi': 31, 't̠ʃʰ': 32}\n",
      "New vocab size:  33\n",
      "Tokenizer for Korean saved.\n",
      "\n",
      "Training tokenizer for Swedish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'ʝ': 5, 'k': 6, 'l': 7, 'ɛ': 8, 'm': 9, 'd̪': 10, 'e': 11, 'ʉ̟': 12, 'f': 13, 'ɪ': 14, 'ŋ': 15, 'ɹ': 16, 'a': 17, 'n̪': 18, 'iː': 19, 'ɑː': 20, 'ɛː': 21, 't̪': 22, 's̪': 23, 'v': 24, 'oː': 25, 'uː': 26, 'eː': 27, 'ʊ': 28, 'p': 29, 'b': 30, 'h': 31, 'øː': 32, 'yː': 33, 'ʂ': 34, 'ɡ': 35, 'ɵ': 36, 'ʃ': 37, 'œ': 38, 'ɕ': 39, 'ʏ': 40, 'ɧ': 41, 'z': 42}\n",
      "Vocab size:  43\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'ʝ': 5, 'k': 6, 'l': 7, 'ɛ': 8, 'm': 9, 'd̪': 10, 'e': 11, 'ʉ̟': 12, 'f': 13, 'ɪ': 14, 'ŋ': 15, 'ɹ': 16, 'a': 17, 'n̪': 18, 'iː': 19, 'ɑː': 20, 'ɛː': 21, 't̪': 22, 's̪': 23, 'v': 24, 'oː': 25, 'uː': 26, 'eː': 27, 'ʊ': 28, 'p': 29, 'b': 30, 'h': 31, 'øː': 32, 'yː': 33, 'ʂ': 34, 'ɡ': 35, 'ɵ': 36, 'ʃ': 37, 'œ': 38, 'ɕ': 39, 'ʏ': 40, 'ɧ': 41, 'z': 42}\n",
      "New vocab size:  43\n",
      "Tokenizer for Swedish saved.\n",
      "\n",
      "Training tokenizer for Norwegian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 't̪ʰ': 4, 'ɑ': 5, 'kː': 6, 'ʋ': 7, 'a': 8, 'ɾ': 9, 'ʃ': 10, 'o̞ː': 11, 'ɡ': 12, 'uː': 13, 'd̪': 14, 'eː': 15, 'e̞': 16, 's': 17, 'h': 18, 'ʉː': 19, 'tː': 20, 'n̪': 21, 'pː': 22, 'ə': 23, 'l': 24, 'ɪ': 25, 'b': 26, 'iː': 27, 'æ': 28, 'j': 29, 'kʰ': 30, 'ʉ': 31, 'ɒ̝': 32, 'm': 33, 'ø̞ː': 34, 'f': 35, 'yː': 36, 'ai': 37, 'pʰ': 38, 'øy': 39, 'ŋ': 40, 'dː': 41, 'œ': 42, 'bː': 43, 'ç': 44, 'æː': 45, 'ɑː': 46, 'ʏ': 47, 'æʉ': 48, 'ʊ': 49, 'ɡː': 50, 'ɔy': 51, 'ʂ': 52, 'w': 53}\n",
      "Vocab size:  54\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 't̪ʰ': 4, 'ɑ': 5, 'kː': 6, 'ʋ': 7, 'a': 8, 'ɾ': 9, 'ʃ': 10, 'o̞ː': 11, 'ɡ': 12, 'uː': 13, 'd̪': 14, 'eː': 15, 'e̞': 16, 's': 17, 'h': 18, 'ʉː': 19, 'tː': 20, 'n̪': 21, 'pː': 22, 'ə': 23, 'l': 24, 'ɪ': 25, 'b': 26, 'iː': 27, 'æ': 28, 'j': 29, 'kʰ': 30, 'ʉ': 31, 'ɒ̝': 32, 'm': 33, 'ø̞ː': 34, 'f': 35, 'yː': 36, 'ai': 37, 'pʰ': 38, 'øy': 39, 'ŋ': 40, 'dː': 41, 'œ': 42, 'bː': 43, 'ç': 44, 'æː': 45, 'ɑː': 46, 'ʏ': 47, 'æʉ': 48, 'ʊ': 49, 'ɡː': 50, 'ɔy': 51, 'ʂ': 52, 'w': 53}\n",
      "New vocab size:  54\n",
      "Tokenizer for Norwegian saved.\n",
      "\n",
      "Training tokenizer for Quechua...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɛ': 4, 'l': 5, 'β': 6, 'ɪ': 7, 'n': 8, 'a': 9, 's': 10, 't': 11, 'r': 12, 'd': 13, 'aː': 14, 't̠ʃ': 15, 'm': 16, 'ɔ': 17, 'h': 18, 'p': 19, 'ʊ': 20, 'ɡ': 21, 'k': 22, 'q': 23, 'f': 24, 'j': 25, 'w': 26, 'ʎ': 27, 'pʼ': 28, 'ʔ': 29, 'tʼ': 30, 't̠ʃʼ': 31, 'kʼ': 32, 'ɪː': 33, 'qʼ': 34, 'ɛː': 35}\n",
      "Vocab size:  36\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɛ': 4, 'l': 5, 'β': 6, 'ɪ': 7, 'n': 8, 'a': 9, 's': 10, 't': 11, 'r': 12, 'd': 13, 'aː': 14, 't̠ʃ': 15, 'm': 16, 'ɔ': 17, 'h': 18, 'p': 19, 'ʊ': 20, 'ɡ': 21, 'k': 22, 'q': 23, 'f': 24, 'j': 25, 'w': 26, 'ʎ': 27, 'pʼ': 28, 'ʔ': 29, 'tʼ': 30, 't̠ʃʼ': 31, 'kʼ': 32, 'ɪː': 33, 'qʼ': 34, 'ɛː': 35}\n",
      "New vocab size:  36\n",
      "Tokenizer for Quechua saved.\n",
      "\n",
      "Training tokenizer for Catalan...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'w': 5, 'ɛ': 6, 'ə': 7, 'ð': 8, 't̪': 9, 'j': 10, 'i': 11, 'ɔ': 12, 'n̺': 13, 'z̺': 14, 'd̪': 15, 's̺': 16, 'β': 17, 'm': 18, 'e': 19, 'f': 20, 'ɾ̺': 21, 'r̺': 22, 'u̯': 23, 'k': 24, 'u': 25, 'b': 26, 'p': 27, 'ɣ': 28, 'ɡ': 29, 'ŋ': 30, 'o': 31, 'ɫ̺': 32, 'ɲ̟': 33, 'ʒ': 34, 'ʃ': 35, 'ʎ̟': 36, 't̠ʃ': 37, 'd̠ʒ': 38, 'ts̺': 39}\n",
      "Vocab size:  40\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'w': 5, 'ɛ': 6, 'ə': 7, 'ð': 8, 't̪': 9, 'j': 10, 'i': 11, 'ɔ': 12, 'n̺': 13, 'z̺': 14, 'd̪': 15, 's̺': 16, 'β': 17, 'm': 18, 'e': 19, 'f': 20, 'ɾ̺': 21, 'r̺': 22, 'u̯': 23, 'k': 24, 'u': 25, 'b': 26, 'p': 27, 'ɣ': 28, 'ɡ': 29, 'ŋ': 30, 'o': 31, 'ɫ̺': 32, 'ɲ̟': 33, 'ʒ': 34, 'ʃ': 35, 'ʎ̟': 36, 't̠ʃ': 37, 'd̠ʒ': 38, 'ts̺': 39}\n",
      "New vocab size:  40\n",
      "Tokenizer for Catalan saved.\n",
      "\n",
      "Training tokenizer for Italian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɛ': 4, 'kː': 5, 'o': 6, 'pː': 7, 'l': 8, 'ɐ': 9, 'n': 10, 'i': 11, 'm': 12, 'k': 13, 's': 14, 't': 15, 'ɔ': 16, 'z': 17, 'f': 18, 'v': 19, 'e': 20, 'd': 21, 'j': 22, 't̠ʃ': 23, 'b': 24, 'w': 25, 'ɛː': 26, 'p': 27, 'r': 28, 'u': 29, 'ɡ': 30, 'ʎ': 31, 'd̠ʒ': 32, 'tː': 33, 'ɐː': 34, 'ts': 35, 'dː': 36, 'oː': 37, 'iː': 38, 'sː': 39, 't̠ʃː': 40, 'ɾ': 41, 'eː': 42, 'dz': 43, 'bː': 44, 'd̠ʒː': 45, 'ɲ': 46, 'tsː': 47, 'ʃ': 48, 'a': 49, 'ɔː': 50, 'dzː': 51, 'ŋ': 52, 'h': 53, 'uː': 54, 'ɡː': 55, 'ʒ': 56}\n",
      "Vocab size:  57\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɛ': 4, 'kː': 5, 'o': 6, 'pː': 7, 'l': 8, 'ɐ': 9, 'n': 10, 'i': 11, 'm': 12, 'k': 13, 's': 14, 't': 15, 'ɔ': 16, 'z': 17, 'f': 18, 'v': 19, 'e': 20, 'd': 21, 'j': 22, 't̠ʃ': 23, 'b': 24, 'w': 25, 'ɛː': 26, 'p': 27, 'r': 28, 'u': 29, 'ɡ': 30, 'ʎ': 31, 'd̠ʒ': 32, 'tː': 33, 'ɐː': 34, 'ts': 35, 'dː': 36, 'oː': 37, 'iː': 38, 'sː': 39, 't̠ʃː': 40, 'ɾ': 41, 'eː': 42, 'dz': 43, 'bː': 44, 'd̠ʒː': 45, 'ɲ': 46, 'tsː': 47, 'ʃ': 48, 'a': 49, 'ɔː': 50, 'dzː': 51, 'ŋ': 52, 'h': 53, 'uː': 54, 'ɡː': 55, 'ʒ': 56}\n",
      "New vocab size:  57\n",
      "Tokenizer for Italian saved.\n",
      "\n",
      "Training tokenizer for PortuguesePt...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'l̪ˠ': 5, 'a': 6, 'p': 7, 'ɐ': 8, 'i': 9, 'n̪': 10, 'e': 11, 'ʃ': 12, 'f': 13, 'ɾ': 14, 'ɐ̃': 15, 'd̪': 16, 'm': 17, 'ʒ': 18, 'b': 19, 'ɯ': 20, 'ɛ': 21, 'ɐ̃i': 22, 'ʁ': 23, 't̪': 24, 's': 25, 'o': 26, 'ɐ̃u̜': 27, 'ũ': 28, 'ɡ': 29, 'u': 30, 'k': 31, 'z': 32, 'au̜': 33, 'ai': 34, 'eu̜': 35, 'ɐi': 36, 'ɲ': 37, 'ɛu̜': 38, 'ĩ': 39, 'ũi': 40, 'ɔi': 41, 'õ': 42, 'õi': 43, 'ẽ': 44, 'v': 45, 'oi': 46, 'ʎ': 47, 'iu̜': 48, 'ui': 49, 'ɛi': 50, 'ts': 51}\n",
      "Vocab size:  52\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'l̪ˠ': 5, 'a': 6, 'p': 7, 'ɐ': 8, 'i': 9, 'n̪': 10, 'e': 11, 'ʃ': 12, 'f': 13, 'ɾ': 14, 'ɐ̃': 15, 'd̪': 16, 'm': 17, 'ʒ': 18, 'b': 19, 'ɯ': 20, 'ɛ': 21, 'ɐ̃i': 22, 'ʁ': 23, 't̪': 24, 's': 25, 'o': 26, 'ɐ̃u̜': 27, 'ũ': 28, 'ɡ': 29, 'u': 30, 'k': 31, 'z': 32, 'au̜': 33, 'ai': 34, 'eu̜': 35, 'ɐi': 36, 'ɲ': 37, 'ɛu̜': 38, 'ĩ': 39, 'ũi': 40, 'ɔi': 41, 'õ': 42, 'õi': 43, 'ẽ': 44, 'v': 45, 'oi': 46, 'ʎ': 47, 'iu̜': 48, 'ui': 49, 'ɛi': 50, 'ts': 51}\n",
      "New vocab size:  52\n",
      "Tokenizer for PortuguesePt saved.\n",
      "\n",
      "Training tokenizer for PortugueseBr...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a': 5, 's̪': 6, 'k': 7, 'ɛ': 8, 'ɾ': 9, 'u': 10, 'b': 11, 'e': 12, 'aʊ̯': 13, 'ɡ': 14, 'ɐ': 15, 'oɪ̯': 16, 'z': 17, 'i': 18, 'õ': 19, 't̪': 20, 'eʊ̯': 21, 'n̪': 22, 'v': 23, 'd̪': 24, 'ɐ̃ʊ̯̃': 25, 'eɪ̯': 26, 'd̠ʒ': 27, 'ẽɪ̯̃': 28, 'p': 29, 'r': 30, 'ɔ': 31, 'o': 32, 'l': 33, 'ɐ̃': 34, 'ĩ': 35, 'f': 36, 'ɲ': 37, 'ũ': 38, 'uɪ̯': 39, 'w': 40, 'ʒ': 41, 'iʊ̯': 42, 'ʃ': 43, 'oʊ̯': 44, 'aɪ̯': 45, 'ɔɪ̯': 46, 'ɣ': 47, 'ɛɪ̯': 48, 'ɛʊ̯': 49, 'ɪ̯': 50}\n",
      "Vocab size:  51\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'a': 5, 's̪': 6, 'k': 7, 'ɛ': 8, 'ɾ': 9, 'u': 10, 'b': 11, 'e': 12, 'aʊ̯': 13, 'ɡ': 14, 'ɐ': 15, 'oɪ̯': 16, 'z': 17, 'i': 18, 'õ': 19, 't̪': 20, 'eʊ̯': 21, 'n̪': 22, 'v': 23, 'd̪': 24, 'ɐ̃ʊ̯̃': 25, 'eɪ̯': 26, 'd̠ʒ': 27, 'ẽɪ̯̃': 28, 'p': 29, 'r': 30, 'ɔ': 31, 'o': 32, 'l': 33, 'ɐ̃': 34, 'ĩ': 35, 'f': 36, 'ɲ': 37, 'ũ': 38, 'uɪ̯': 39, 'w': 40, 'ʒ': 41, 'iʊ̯': 42, 'ʃ': 43, 'oʊ̯': 44, 'aɪ̯': 45, 'ɔɪ̯': 46, 'ɣ': 47, 'ɛɪ̯': 48, 'ɛʊ̯': 49, 'ɪ̯': 50}\n",
      "New vocab size:  51\n",
      "Tokenizer for PortugueseBr saved.\n",
      "\n",
      "Training tokenizer for Romanian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'ä': 5, 'n̪': 6, 'd̠ʒ': 7, 'i': 8, 'v': 9, 'e̞': 10, 'h': 11, 'u': 12, 'ʒ': 13, 'd̪': 14, 'o̞': 15, 'l': 16, 'ɾ̪': 17, 't̠ʃ': 18, 'p': 19, 'j': 20, 's̪': 21, 'oʊ': 22, 't̪': 23, 'aɪ': 24, 'k': 25, 'w': 26, 'ɡ': 27, 'b': 28, 't̠ʃʲ': 29, 'e̯ä': 30, 'ʃ': 31, 'ʃʲ': 32, 'ə': 33, 'o̯ä': 34, 'ɨ': 35, 'uɪ': 36, 'f': 37, 't̪s̪': 38, 'z̪': 39, 'əɪ': 40, 'eɪ': 41, 'tsʲ': 42, 'zʲ': 43, 'iɪ': 44, 'aʊ': 45, 'tʲ': 46, 'nʲ': 47, 'eʊ': 48, 'iʊ': 49, 'ɾʲ': 50, 'mʲ': 51, 'bʲ': 52, 'sʲ': 53, 'kʲ': 54, 'lʲ': 55, 'eo': 56, 'd̠ʒʲ': 57, 'dʲ': 58, 'pʲ': 59, 'əʊ': 60, 'fʲ': 61, 'oɪ': 62}\n",
      "Vocab size:  63\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'ä': 5, 'n̪': 6, 'd̠ʒ': 7, 'i': 8, 'v': 9, 'e̞': 10, 'h': 11, 'u': 12, 'ʒ': 13, 'd̪': 14, 'o̞': 15, 'l': 16, 'ɾ̪': 17, 't̠ʃ': 18, 'p': 19, 'j': 20, 's̪': 21, 'oʊ': 22, 't̪': 23, 'aɪ': 24, 'k': 25, 'w': 26, 'ɡ': 27, 'b': 28, 't̠ʃʲ': 29, 'e̯ä': 30, 'ʃ': 31, 'ʃʲ': 32, 'ə': 33, 'o̯ä': 34, 'ɨ': 35, 'uɪ': 36, 'f': 37, 't̪s̪': 38, 'z̪': 39, 'əɪ': 40, 'eɪ': 41, 'tsʲ': 42, 'zʲ': 43, 'iɪ': 44, 'aʊ': 45, 'tʲ': 46, 'nʲ': 47, 'eʊ': 48, 'iʊ': 49, 'ɾʲ': 50, 'mʲ': 51, 'bʲ': 52, 'sʲ': 53, 'kʲ': 54, 'lʲ': 55, 'eo': 56, 'd̠ʒʲ': 57, 'dʲ': 58, 'pʲ': 59, 'əʊ': 60, 'fʲ': 61, 'oɪ': 62}\n",
      "New vocab size:  63\n",
      "Tokenizer for Romanian saved.\n",
      "\n",
      "Training tokenizer for Serbian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'j': 4, 'e̞': 5, 's̪̻': 6, 't̪̻': 7, 'u': 8, 'l': 9, 'o̞': 10, 'ʒ̺': 11, 'i': 12, 'ʋ': 13, 'd̪̻': 14, 'ä': 15, 'm': 16, 'n': 17, 'r': 18, 'k': 19, 't̪̻s̪̻': 20, 'p': 21, 'ʃ̺': 22, 'x': 23, 'b': 24, 'ɡ': 25, 't̻ʃ̻': 26, 'f': 27, 'z̪̻': 28, 'ɲ': 29, 'ʎ': 30, 'd̻ʒ̻': 31, 'y': 32, 'w': 33}\n",
      "Vocab size:  34\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'j': 4, 'e̞': 5, 's̪̻': 6, 't̪̻': 7, 'u': 8, 'l': 9, 'o̞': 10, 'ʒ̺': 11, 'i': 12, 'ʋ': 13, 'd̪̻': 14, 'ä': 15, 'm': 16, 'n': 17, 'r': 18, 'k': 19, 't̪̻s̪̻': 20, 'p': 21, 'ʃ̺': 22, 'x': 23, 'b': 24, 'ɡ': 25, 't̻ʃ̻': 26, 'f': 27, 'z̪̻': 28, 'ɲ': 29, 'ʎ': 30, 'd̻ʒ̻': 31, 'y': 32, 'w': 33}\n",
      "New vocab size:  34\n",
      "Tokenizer for Serbian saved.\n",
      "\n",
      "Training tokenizer for Polish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'e': 4, 'd̪': 5, 'l̪': 6, 'v': 7, 'o': 8, 'w': 9, 'a': 10, 'j': 11, 'b': 12, 'r': 13, 'ɲ': 14, 'i': 15, 'ɕ': 16, 'u': 17, 'x': 18, 'tɕ': 19, 't̪': 20, 'k': 21, 'p': 22, 'ɨ': 23, 'dʑ': 24, 'z̪': 25, 'n̪': 26, 'f': 27, 'ʑ': 28, 'm': 29, 'z̻': 30, 's̻': 31, 't̻s̻': 32, 't̪s̪': 33, 'ɡ': 34, 's̪': 35, 'ŋ': 36, 'kʲ': 37, 't': 38, 'ɡʲ': 39, 'ɣ': 40, 'ẽ': 41, 'd̻z̻': 42}\n",
      "Vocab size:  43\n",
      "Using only primary stress markers...\n",
      "New vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'e': 4, 'd̪': 5, 'l̪': 6, 'v': 7, 'o': 8, 'w': 9, 'a': 10, 'j': 11, 'b': 12, 'r': 13, 'ɲ': 14, 'i': 15, 'ɕ': 16, 'u': 17, 'x': 18, 'tɕ': 19, 't̪': 20, 'k': 21, 'p': 22, 'ɨ': 23, 'dʑ': 24, 'z̪': 25, 'n̪': 26, 'f': 27, 'ʑ': 28, 'm': 29, 'z̻': 30, 's̻': 31, 't̻s̻': 32, 't̪s̪': 33, 'ɡ': 34, 's̪': 35, 'ŋ': 36, 'kʲ': 37, 't': 38, 'ɡʲ': 39, 'ɣ': 40, 'ẽ': 41, 'd̻z̻': 42}\n",
      "New vocab size:  43\n",
      "Tokenizer for Polish saved.\n"
     ]
    }
   ],
   "source": [
    "for language, dataset in datasets.items():\n",
    "    print(f'\\nTraining tokenizer for {language}...')\n",
    "    allow_non_phoible = language in ['Mandarin', 'Cantonese'] # For Mandarin and Cantonese, allow non-phoible tokens since we merge tone with vowels\n",
    "    vocab = build_vocabulary([dataset], allow_non_phoible=allow_non_phoible, allow_stressed_tokens=True)\n",
    "    tokenizer = build_phoneme_tokenizer(vocab, add_stress_replacer=True)\n",
    "    # save locally\n",
    "    tokenizer.save_pretrained(f'ipa-childes-tokenizers/{language}')\n",
    "    print(f'Tokenizer for {language} saved.')\n",
    "\n",
    "# print(f'\\nTrainking tokenizer for all languages...')\n",
    "# vocab = build_vocabulary(datasets.values())\n",
    "# tokenizer = build_phoneme_tokenizer(vocab)\n",
    "# tokenizer.push_to_hub(\"phonemetransformers/CHILDES-phoneme-tokenizer\")\n",
    "# print('Done.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnglishNA tokenizer is ok: True\n",
      "EnglishUK tokenizer is ok: True\n",
      "French tokenizer is ok: True\n",
      "German tokenizer is ok: True\n",
      "Spanish tokenizer is ok: True\n",
      "Dutch tokenizer is ok: True\n",
      "Mandarin tokenizer is ok: True\n",
      "Japanese tokenizer is ok: True\n",
      "Cantonese tokenizer is ok: True\n",
      "Estonian tokenizer is ok: True\n",
      "Croatian tokenizer is ok: True\n",
      "Danish tokenizer is ok: True\n",
      "Basque tokenizer is ok: True\n",
      "Hungarian tokenizer is ok: True\n",
      "Turkish tokenizer is ok: True\n",
      "Farsi tokenizer is ok: True\n",
      "Icelandic tokenizer is ok: True\n",
      "Indonesian tokenizer is ok: True\n",
      "Irish tokenizer is ok: True\n",
      "Welsh tokenizer is ok: True\n",
      "Korean tokenizer is ok: True\n",
      "Swedish tokenizer is ok: True\n",
      "Norwegian tokenizer is ok: True\n",
      "Quechua tokenizer is ok: True\n",
      "Catalan tokenizer is ok: True\n",
      "Italian tokenizer is ok: True\n",
      "PortuguesePt tokenizer is ok: True\n",
      "PortugueseBr tokenizer is ok: True\n",
      "Romanian tokenizer is ok: True\n",
      "Serbian tokenizer is ok: True\n",
      "Polish tokenizer is ok: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def check_tokenizer(tokenizer):\n",
    "    # It turns out that the Whitespace normalizer does not include tone symbols, so for the Cantonese \n",
    "    # and Mandarin tokenizers, it was splitting phonemes like 'a˥' in two, and so converting them to two UNK\n",
    "    # tokens. This is fixed by using WhitespaceSplit normalizer, which works like split().\n",
    "    is_ok = True\n",
    "    for v, x in tokenizer.vocab.items():\n",
    "        if not (tokenizer.encode(v)[1:] == [x]):\n",
    "            #print(f'Tokenizer failed to encode \"{v}\", gave {tokenizer.encode(v)[1:]}')\n",
    "            is_ok = False\n",
    "    return is_ok\n",
    "\n",
    "for language in datasets.keys():\n",
    "    t = AutoTokenizer.from_pretrained(f'ipa-childes-tokenizers/{language}')\n",
    "    is_ok = check_tokenizer(t)\n",
    "    print(f'{language} tokenizer is ok: {is_ok}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenizers for CHILDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('phonemetransformers/IPA-CHILDES', 'English', split='train')\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "        ]\n",
    "    )\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"UTT_BOUNDARY\", \"PAD\", \"UNK\"])\n",
    "tokenizer.train_from_iterator(dataset['processed_gloss'], trainer=trainer)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: is that what you saw?\n",
      "['UTT_BOUNDARY', 'Ġis', 'Ġthat', 'Ġwhat', 'Ġyou', 'Ġsaw', '?']\n"
     ]
    }
   ],
   "source": [
    "example = dataset['processed_gloss'][300]\n",
    "encoding = tokenizer.encode(example)\n",
    "print(f'Example: {example}')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/phonemetransformers/CHILDES-English-BPE-gloss-tokenizer/commit/dc70201e9f3dc609aea522ae4df6cc435f07a55e', commit_message='Upload tokenizer', commit_description='', oid='dc70201e9f3dc609aea522ae4df6cc435f07a55e', pr_url=None, repo_url=RepoUrl('https://huggingface.co/phonemetransformers/CHILDES-English-BPE-gloss-tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='phonemetransformers/CHILDES-English-BPE-gloss-tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, pad_token='PAD', unk_token='UNK', bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', add_prefix_space=True)\n",
    "wrapped_tokenizer.push_to_hub(\"phonemetransformers/CHILDES-English-BPE-gloss-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 115, 92, 95, 67, 781, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(example, padding='max_length', max_length=20, truncation=True, add_special_tokens=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UTT_BOUNDARY',\n",
       " 'Ġis',\n",
       " 'Ġthat',\n",
       " 'Ġwhat',\n",
       " 'Ġyou',\n",
       " 'Ġsaw',\n",
       " '?',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 124, 115, 61, 3630, 45, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer('this is a test .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
